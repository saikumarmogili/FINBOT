# -*- coding: utf-8 -*-
"""finbot1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qb8fZR5iPDsHZVa1gJD48S7wDRl6j7hB
"""

!pip install pyspark

from google.colab import drive
drive.mount('/content/drive')

import zipfile

zip_path = "/content/complaints.csv.zip"
extract_path = "/content/"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

import pandas as pd
from collections import defaultdict

csv_path = "/content/complaints.csv"
chunksize = 10000
target_column = "Consumer complaint narrative"
label_column = "Product"
sub_product_column = "Sub-product"
issue_column = "Issue"
min_text_length = 50

# Dataset configurations: (label_count * samples_per_label = total rows)
configs = {
    "120K": {"top_n_classes": 20, "max_samples_per_class": 6000},
}

# Load once to get top N classes
full_df = pd.read_csv(csv_path, usecols=[label_column])
full_class_counts = full_df[label_column].value_counts()

for label, config in configs.items():
    print(f"\nüöÄ Generating {label} dataset...")

    top_classes = full_class_counts.head(config["top_n_classes"]).index.tolist()
    class_counts = defaultdict(int)
    cleaned_data = []

    # Iterate over CSV in chunks to process large dataset
    for chunk in pd.read_csv(csv_path, chunksize=chunksize, usecols=[label_column, sub_product_column, issue_column, target_column]):
        chunk = chunk.dropna(subset=[target_column, label_column, sub_product_column, issue_column])

        # Process each row in the chunk
        for _, row in chunk.iterrows():
            text = str(row[target_column]).strip()
            label_val = row[label_column]
            sub_product_val = row[sub_product_column]
            issue_val = row[issue_column]

            if (
                label_val in top_classes and
                class_counts[label_val] < config["max_samples_per_class"] and
                len(text) >= min_text_length
            ):
                # Create the cleaned entry
                cleaned_data.append({
                    "Product": label_val,
                    "Sub-product": sub_product_val,
                    "Issue": issue_val,
                    "Complaint Narrative": text
                })
                class_counts[label_val] += 1

        # Stop early if all class targets met
        if all(class_counts[c] >= config["max_samples_per_class"] for c in top_classes):
            break

    # Save each version
    df_cleaned = pd.DataFrame(cleaned_data)
    out_path = f"cleaned_complaints_{label.lower()}.csv"
    df_cleaned.to_csv(out_path, index=False)
    print(f"‚úÖ Saved: {out_path} with shape {df_cleaned.shape}")

import pandas as pd

df_cleaned = pd.read_csv("/content/cleaned_complaints_120k.csv")
df_cleaned.head()

!pip install -U sentence-transformers
!pip install boto3  # only if you plan to upload to AWS S3

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import os

# Load your dataset (change path if needed)
df = pd.read_csv('/content/cleaned_complaints_120k.csv')
texts = df['Complaint Narrative'].dropna().tolist()

model = SentenceTransformer('all-MiniLM-L6-v2')

def embed_batch(text_batch):
    return model.encode(text_batch, batch_size=32)

def batchify(lst, n):
    """Split list into chunks of n."""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

batch_size = 1000
batches = list(batchify(texts, batch_size))

all_embeddings = []

for i, batch in enumerate(tqdm(batches)):
    emb = embed_batch(batch)
    all_embeddings.extend(emb)
    # Optional: Save intermediate batches (for recovery)
    np.save(f'/content/embeddings_batch_{i}.npy', emb)

# Final embedding array
all_embeddings = np.array(all_embeddings)
np.save('/content/final_complaint_embeddings.npy', all_embeddings)

from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
from tqdm import tqdm
import os

# Load model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Load data
df = pd.read_csv('/content/cleaned_complaints_120k.csv')

# Columns to embed
columns_to_embed = ['Product', 'Issue', 'Sub-product']
batch_size = 5000

# Create folder to store embeddings
os.makedirs("/content/bert_batches", exist_ok=True)

def process_column_in_batches(col_name):
    print(f"\nüîÅ Embedding column: {col_name}")
    texts = df[col_name].fillna("").astype(str).tolist()

    all_embeddings = []

    for i in tqdm(range(0, len(texts), batch_size)):
        batch_texts = texts[i:i+batch_size]
        batch_embeddings = model.encode(batch_texts, batch_size=32, show_progress_bar=False)
        all_embeddings.extend(batch_embeddings)

        # Save individual batch file
        batch_path = f"/content/bert_batches/{col_name.lower().replace(' ', '_')}_batch_{i//batch_size}.npy"
        np.save(batch_path, batch_embeddings)

    # Save combined embeddings for column
    final_path = f"/content/bert_batches/{col_name.lower().replace(' ', '_')}_full.npy"
    np.save(final_path, all_embeddings)
    print(f"‚úÖ Final saved: {final_path} | Shape: {np.array(all_embeddings).shape}")

# Process each column
for col in columns_to_embed:
    process_column_in_batches(col)

import numpy as np

# Load the saved file
embeddings = np.load('/content/final_complaint_embeddings.npy')

# Show first 3 embeddings
for i in range(3):
    print(f"Embedding {i+1}:")
    print(embeddings[i])
    print("------")

import pandas as pd
import numpy as np

# Load BERT embeddings of complaint narratives
X_narrative = np.load('/content/final_complaint_embeddings.npy')

# Load and prepare main dataset
complaints_df = pd.read_csv('/content/cleaned_complaints_120k.csv')
complaints_df = complaints_df[['Complaint Narrative', 'Product', 'Sub-product', 'Issue']].dropna()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report

# Encode Product
le_product = LabelEncoder()
y_product = le_product.fit_transform(complaints_df['Product'])

# Split and train
X_train_prod, X_test_prod, y_train_prod, y_test_prod = train_test_split(
    X_narrative, y_product, test_size=0.2, random_state=42)

clf_product = LogisticRegression(max_iter=1000)
clf_product.fit(X_train_prod, y_train_prod)

# Evaluate
print("üéØ Product Classification Report")
print(classification_report(y_test_prod, clf_product.predict(X_test_prod), target_names=le_product.classes_))

from collections import defaultdict

subproduct_models = defaultdict(dict)

for product in complaints_df['Product'].unique():
    sub_df = complaints_df[complaints_df['Product'] == product]
    X_sub = X_narrative[sub_df.index]

    le_sub = LabelEncoder()
    y_sub = le_sub.fit_transform(sub_df['Sub-product'])

    clf_sub = LogisticRegression(max_iter=1000)
    clf_sub.fit(X_sub, y_sub)

    subproduct_models[product]['model'] = clf_sub
    subproduct_models[product]['encoder'] = le_sub

issue_models = defaultdict(dict)

for sub_product in complaints_df['Sub-product'].unique():
    issue_df = complaints_df[complaints_df['Sub-product'] == sub_product]
    X_issue = X_narrative[issue_df.index]

    le_issue = LabelEncoder()
    y_issue = le_issue.fit_transform(issue_df['Issue'])

    clf_issue = LogisticRegression(max_iter=1000)
    clf_issue.fit(X_issue, y_issue)

    issue_models[sub_product]['model'] = clf_issue
    issue_models[sub_product]['encoder'] = le_issue

from sentence_transformers import SentenceTransformer

embed_model = SentenceTransformer('all-MiniLM-L6-v2')

def predict_full_pipeline(user_input):
    embedding = embed_model.encode([user_input])

    # Predict Product
    pred_prod_enc = clf_product.predict(embedding)[0]
    pred_prod = le_product.inverse_transform([pred_prod_enc])[0]
    print(f"üß≠ Product: {pred_prod}")

    # Predict Sub-product
    sub_model = subproduct_models[pred_prod]['model']
    sub_encoder = subproduct_models[pred_prod]['encoder']
    pred_sub_enc = sub_model.predict(embedding)[0]
    pred_sub = sub_encoder.inverse_transform([pred_sub_enc])[0]
    print(f"üîç Sub-product: {pred_sub}")

    # Predict Issue
    if pred_sub in issue_models:
        issue_model = issue_models[pred_sub]['model']
        issue_encoder = issue_models[pred_sub]['encoder']
        pred_issue_enc = issue_model.predict(embedding)[0]
        pred_issue = issue_encoder.inverse_transform([pred_issue_enc])[0]
        print(f"‚ùó Issue: {pred_issue}")
    else:
        pred_issue = "Unknown"
        print("‚ö†Ô∏è Issue prediction not available for this sub-product.")

    return pred_prod, pred_sub, pred_issue

user_input = "I was charged twice on my credit card and no one helped me resolve it."
predict_full_pipeline(user_input)

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min

# STEP 1: Load Data and Embeddings
complaints_df = pd.read_csv('/content/cleaned_complaints_120k.csv')
complaints_df = complaints_df[['Complaint Narrative', 'Product', 'Sub-product', 'Issue']].dropna()

# Load pre-generated narrative embeddings
X_narrative = np.load('/content/final_complaint_embeddings.npy')

# Make sure rows match (dropna affects size)
X_narrative = X_narrative[complaints_df.index]

# STEP 2: Apply KMeans Clustering (50 semantic groups)
n_clusters = 50
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
cluster_labels = kmeans.fit_predict(X_narrative)

# STEP 3: Get Closest Points to Cluster Centers (Top 50 distinct complaints)
closest_indices, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, X_narrative)

# STEP 4: Extract Representative Complaints
top_50_semantic = complaints_df.iloc[closest_indices][['Product', 'Sub-product', 'Issue']].copy()
top_50_semantic['Cluster'] = range(1, 51)

# STEP 5: Save Output
top_50_semantic.to_csv('/content/top_50_semantic_clusters.csv', index=False)
print("‚úÖ Saved: /content/top_50_semantic_clusters.csv")
display(top_50_semantic.head())



